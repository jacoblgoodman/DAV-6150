{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# DAV 6150 Module 10: Naive Bayes Classifiers\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2 Comments\n",
    "\n",
    "\n",
    "### Don't rely too heavily on metrics like Inertia or Silhouette Scores\n",
    "\n",
    "Clustering metrics + tools like __inertia__ and __silhouette scores__ and __dendrograms__ can be useful for purposes of helping us get some insight into how data we are working with may be structured. \n",
    "\n",
    "However, __we should NEVER rely solely on such metrics when deciding how many groupings we want to impose on a data set__. We __ALWAYS__ need to rely __MUCH__ more heavily on our __domain knowledge__ and make sure we are __considering the business and/or scientific objective__ that has led us to consider the application of clustering methods to a data set.\n",
    "\n",
    "### How to compare V_Revenue values to results of clustering?\n",
    "\n",
    "It is not valid to compare the \"accuracy\" calculated by comparing the cluster grouping labels vs. the __V_Revenue__ values to the null error derived from the labels assigned by the clustering algorithm. That null error rate is derived from (and is specific to) the groupings created by the clustering algorithm. Therefore, it should __NOT__ be used to assess whether the __V_Revenue__ values match the results of the clustering algorithm.\n",
    "\n",
    "In a \"real world\" situation in which use of clustering is considered necessary it would be extremely unlikely that we would not have ANY information on the \"actual\" values (e.g., __V_Revenue__ ) of the intended grouping labels.\n",
    "\n",
    "#### So what could we have done in Project 2?\n",
    "\n",
    "A far better method of comparison would have been to __compute a confusion matrix__ that compares the grouping labels to the __V_Revenue__ values. The content of the confusion matrix can then be used to assess how well the results of the K=2 clustering algorithm match up to the actual __V_Revenue__ values. \n",
    "\n",
    "*** But __REMEMBER__: In a real world scenario in which use of clustering is deemed necessary it is __highly unlikely__ that you will have any pre-existing labels that you can use to assess the efficacy of a clustering algorithm. We have used the pre-existing labels here in Project 2 __solely for the purpose of helping you build intution__ about how clustering algorithms work. Clustering algorithms __are not predictive models__ and their output is not assessed on the basis of categorization metrics. Clustering algorithms merely __impose a given number of groupings on a data set__.\n",
    "\n",
    "\n",
    "### How to treat \"Month\" and other cyclical time-based variables\n",
    "\n",
    "Time-based variables such as day of the week, month, hour, minute, second, etc. are all examples of __cyclical__ (a.k.a. \"circular\") temporal attributes. They are \"cyclical\" since their values will repeat themselves after some fixed __interval__ of time.\n",
    "\n",
    "#### So how should we treat such variables when preparing data for use within a machine learning model?\n",
    "\n",
    "Should we treat cyclical time-based variables as nominal categoricals? Ordinal categoricals? Continuous variables?\n",
    "\n",
    "The answer __depends on the context of the data + the objectives of your work__.\n",
    "\n",
    "For example, if the objective of our work is assessing temporal events wherein the __sequencing of the events__ is considered highly relevant (e.g., __time series analysis__), we might want to treat cyclical variables (or combinations thereof, e.g., day + month + year, etc.) as having __ordinal properties__ or __continuous properties__ that should be retained for purposes of training a machine learning model. \n",
    "\n",
    "By contrast, if the __sequencing of events is not important__ (as is the case with the Project 2 data set), the cyclical properties may not be relevant to us at all, though the labeling of events as having occurred at __different moments in time__ MAY be relevant. In such cases we are free to treat cyclical variables as __nominal categorical variables__, wherein their values are treated as labels indicative of __different moments in time__ but whose sequencing is of no real interest to us for modeling purposes.\n",
    "\n",
    "## REMEMBER: \n",
    "\n",
    "In Data Science (as in statistics), we should __refrain from blindly following any so-called \"rules\"__. We __ALWAYS__ need to make our decisions __relative to the context of the problem at hand__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers Explained\n",
    "\n",
    "__Naive Bayes Classifiers__ are __supervised learning algorithms__ that assume the __conditional independence of all features within an observation__.\n",
    "\n",
    "\n",
    "Naive Bayes Classifiers make use of __Bayes Theorem__ for purposes of assigning probabilistic estimates of the proper classification for a previously unseen observation.  \n",
    "\n",
    "\n",
    "Bayes Theorem allows us to answer the question: __\"How much should you trust your evidence?\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "## $ P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$\n",
    "\n",
    "__Explanation__: Find the probability of an event $A$ happening (our __hypothesis__) given that $B$ (our __evidence__) has already occurred.\n",
    "\n",
    "\n",
    "__P(A|B)__: Represents the __posterior probability__, i.e., the likelihood that the model accurately reflects the probability of $A$ given that $B$ has occurred\n",
    "\n",
    "\n",
    "__P(A)__: Represents the __prior probability__, i.e., the degree to which we believe the model accurately describes reality based on all available prior information\n",
    "\n",
    "\n",
    "__P(B|A)__: Represents the __likelihood__, i.e., a measure of how well the model actually predicts our response variable.\n",
    "\n",
    "\n",
    "__P(B)__: Represents the __normalizing constant__, i.e., a constant value that ensures that the posterior probability density function will integrate to a value of $1$\n",
    "\n",
    "\n",
    "__Conditional Independence__: Bayes Theorem assumes that all explanatory variables are __independent__ from one another, i.e., the presence of any given explanatory variable value is __NOT__ dependent on the presence of any other particular explanatory variable value within a given observation. This means that __every explanatory variable is assumed to have an equivalent amount of effect on the outcome of the classifier__.\n",
    "\n",
    "\n",
    "While this is a __simplifying assumption__ for purposes of minimizing the complexity of the Naive Bayes approach, it very often __is not representative of the actual content of a given data set__, since we quite often are able to discern tangible correlations between explanatory variables. \n",
    "\n",
    "\n",
    "Nevertheless, the Naive Bayes approach is often as effective as other more complex types of machine learning models. \n",
    "\n",
    "\n",
    "\n",
    "## How it Works\n",
    "\n",
    "- Each feature is assumed to be normally distributed + conditionally independent\n",
    "\n",
    "\n",
    "- The likelihood of an observation having a specific classification value is simply the product of the probabilities of the individual explanatory variables having certain values (e.g. the \"play golf? scenario shown in the Module 10 Naive Bayes intro video (https://www.youtube.com/watch?v=CPqOCI0ahss&list=PL_Nji0JOuXg2udXfS6nhK3CkIYLDtHNLp&index=8) and re-created in tabular format here (https://scienceprog.com/simple-explanation-of-naive-bayes-classifier/):  \n",
    "\n",
    "$P(Play|Sunny) =  ( P( Sunny | Play) * P(Play) ) / P (Sunny) $\n",
    "\n",
    "\n",
    "## Types of Naive Bayes Classifiers\n",
    "\n",
    "- __Gaussian__: Used when the explanatory variables are continuous numeric values and follow a normal (a.k.a., \"Gaussian\") distribution\n",
    "\n",
    "\n",
    "- __Multinomial__: Mostly used for document classification problems. Uses word frequency counts as the explanatory variables\n",
    "\n",
    "\n",
    "- __Bernoulli__: Similar to multinomial algorithm; Explanatory variables are boolean values instead of frequency counts, e.g., does a word occur within a document?\n",
    "\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Fast and scalable: requires relatively little in the way of CPU + RAM resources + scales linearly with the number of explanatory variables + observations.\n",
    "\n",
    "\n",
    "- Relatively easy to implement, understand and interpret\n",
    "\n",
    "\n",
    "- Can be very effective when applied to small data sets that have a relatively large number of features (e.g., images, text, speech data) since the conditional independence assumption makes it very unlikely that it will produce a model with high variance.\n",
    "\n",
    "\n",
    "- Can readily handle missing data values within a given feature\n",
    "\n",
    "\n",
    "- If the conditional independence assumptions hold true for the data, a Naive Bayes model is highly likely to outperform other types of classification algorithms (e.g., logistic regression)\n",
    "\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "- Conditional independence assumption eliminates any possibility of capturing any interaction between explanatory variables\n",
    "\n",
    "\n",
    "- Can perform poorly if numeric explanatory variables do not actually follow a normal distribution\n",
    "\n",
    "\n",
    "- Can easily produce an ineffective model if the training data is skewed / not sufficiently representative of the class distributions found within the overall population / data set.\n",
    "\n",
    "\n",
    "- The __Zero Frequency__ problem: If an observation contained within the testing data set contains a classification value that was for some reason not present in the training data set, the Naive Bayes classifier will not be able to make any prediction for that observation. \n",
    "\n",
    "\n",
    "## Common Applications\n",
    "\n",
    "- Medical diagnostics\n",
    "\n",
    "\n",
    "- Sentiment Analysis (i.e., classifying data according to the “emotional tone” of its content, e.g., text classification, spam filters, etc.)\n",
    "\n",
    "\n",
    "- Recommender Systems (e.g., Amazon, Google Search, Netflix, etc.)\n",
    "\n",
    "\n",
    "- Real-time predictions\n",
    "\n",
    "\n",
    "- Multi-class predictions\n",
    "\n",
    "\n",
    "## How to Implement a Naive Bayes Classifier in Python\n",
    "\n",
    "The __scikit-learn__ library includes pre-built functions for each of the types of Naive Bayes classifiers mentioned above:\n",
    "\n",
    "- __Gaussian__: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "\n",
    "\n",
    "- __Multinomial__: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "\n",
    "- __Bernoulli__: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html\n",
    "\n",
    "\n",
    "An example of how to use a Naive Bayes classifier for text classification from the Module 10 assigned readings: https://medium.com/analytics-vidhya/naive-bayes-classifier-for-text-classification-556fabaf252b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Using Naive Bayes\n",
    "\n",
    "An example of how to implement sentiment analysis using Naive Bayes concepts from the Module 10 assigned readings: https://streamsql.io/blog/sentiment-analysis\n",
    "\n",
    "An example of how to apply sentiment analysis using Naive Bayes concepts via the Python NLTK library from the Module 10 assigned readings: http://blog.chapagain.com.np/python-nltk-sentiment-analysis-on-movie-reviews-natural-language-processing-nlp/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
